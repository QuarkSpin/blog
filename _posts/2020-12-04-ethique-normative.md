---
layout: post
title: "Éthique Normative"
date:   2020-12-04 22:24:00 +0200
categories: Philosophie Éthique
tags:
    - Philosophie morale
    - Éthique normative
excerpt: La société moderne se pose de nombreuses questions éthiques, notamment dans le cadre de la bioéthique et des nouvelles technologies. Mais avant toute chose, il me faut conduire une réflexion sérieuse sur l'*éthique normative*, nécessaire pour aborder des cas concrets avec rigueur.
image:
  path: /images/post-images/2020-12-04-ethique-normative/main.jpg
  thumbnail: /images/post-images/2020-12-04-ethique-normative/main-thumb-flat.jpg
  caption: " Photo par [Jordan Madrid](https://unsplash.com/@jordanmadrid)"
---

{% include toc %}

## Introduction

### Questions d'éthique

La société moderne se pose de nombreuses questions éthiques, notamment dans le cadre de la bioéthique et des nouvelles technologies.

* Est-ce que les personnes vivant actuellement importent plus que les personnes à venir ?
* Peut-on, et si oui comment, trier les malades et blessés en cas de ressources limitées ?
* L'avortement est-il justifiable dans certaines circonstances ? Et l'euthanasie ?
* Peut-on modifier le génome humain pour soigner le cancer ?
* La justice doit-elle être punitive, réhabilitative ou réparatrice ?
* Vivre dans le monde "réel" plutôt que "virtuel" est-il une obligation éthique ?
* Qu'est-ce qui peut ou non justifier une intervention militaire ?
* Sur quels critères se baser pour permettre ou interdire des comportements en société ?
* Quelles sont les problématiques autour des recherches sur l'embryon humain ?
* Doit-on présumer le consentement au don d'organe d'une personne décédée ?
* Quelles questions éthiques entourent la contraception, la reproduction et l'éducation sexuelle ?
* Quels rôles et règles de décision donner aux IA, par exemple dans le cas d'une voiture autonome (doit-elle tenter de sauver ses occupants ou les sacrifier pour sauver plus de vies) ?
* Etc., etc.

Je souhaite aborder toutes ces questions et d'autres dans des articles à venir, plus ou moins en profondeur selon les sujets. Mais avant toute chose, il me faut conduire une réflexion sérieuse sur l'*éthique normative*, nécessaire pour aborder des cas concrets avec rigueur.

### Dilemmes

Commençons par deux expériences de pensée.

Imaginez un aiguilleur d'une voie de train. Il peut décider de faire passer le train par la voie principale ou par une voie secondaire. Imaginez que cinq personnes soient coincées sur la voie principale et qu'une soit coincée sur la voie secondaire. Un train arrive et ses freins ont lâché. L'aiguilleur peut alors :

1. Ne rien faire et laisser le train s'engager sur la voie principale. Le train tuera cinq personnes.
2. Aiguiller le train sur la voie secondaire. Le train tuera une personne.

Que devrait faire l'aiguilleur ? Quel est le choix le plus éthique ?

Il me semble assez intuitif de privilégier la deuxième option afin de sauver le plus de vies.

![Dilemme du tramway]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/dilemme_tram.svg)

Imaginez maintenant un chirurgien, spécialiste des transplantations.

Pour l'expérience, considérons que :

* Le taux de réussite du chirurgien est de 100 %, les opérations se font sans risque.
* Le chirurgien n'a de comptes à rendre à personne, si ce n'est lui-même, personne n'est au courant de ses choix éthiques et il ne fait face à aucune conséquence juridique, économique, sociale, etc.
* Son service est "isolé", il ne peut pas demander de l'aide, transférer des patients, recevoir des organes d'une banque d'organes, etc.

![Chirurgien]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/chirurgien.jpg){: .medium height}
*Photo du [National Cancer Institute](https://unsplash.com/@nci)*

Un jour un patient arrive pour une appendicite, une opération bénigne. Le chirurgien l'endort et s'apprête à l'opérer quand cinq autres patients arrivent, chacun ayant un organe vital différent défaillant. Il se trouve que l'homme qui doit être anesthésié pour l'appendicite est compatible avec les cinq nouveaux patients, ses organes pourraient les sauver.

Le chirurgien a alors deux choix :

1. Soit il opère le premier patient et laisse mourir les cinq autres
2. Soit il prélève les organes du premier patient sans son consentement, causant sa mort, et sauve les cinq autres

Que devrait faire le chirurgien ? Quel est le choix le plus éthique ?

Si comme dans le premier scénario, on souhaite sauver le plus grand nombre de vie, alors la seconde option est la bonne. Pourtant la choisiriez-vous ?

Les deux dilemmes précédents sont des expériences de pensée proposées, parmi d'autres, par la philosophe américaine Judith Jarvis Thomson notamment dans deux articles[^thomson_76] [^thomson_85] publiés en 1976 et 1985. Thomson a posé ses problèmes à un grand nombre de participants et recueilli les résultats.

Il est intéressant de noter que dans les deux scénarios que j'ai mentionnés, à chaque fois on a le choix de faire soit une action qui ôte une vie pour en sauver cinq, soit l'inverse. Mais d'après ces études, la majorité des gens choisit la deuxième option dans le premier cas (dévier le train), mais la première dans le second cas (ne pas prélever les organes).

Pourtant les deux cas sont similaires :

* *Ne pas dévier le train* correspond à *ne pas prélever d'organes* (5 morts, 1 vivant) : on laisse les choses se dérouler comme si on n'avait pas eu de solution alternative
* *Dévier le train* correspond à *prélever les organes* (5 vivants, 1 mort) : on agit pour sauver le plus grand nombre, causant la mort d'une personne qui vivrait sans notre intervention

Déterminer comment choisir ce qui est éthique (ou *moral*, ces termes pouvant être confondus dans ce contexte[^britannica_morality_ethics]) de faire est justement le rôle de l'*éthique normative*, sujet de cet article.

### Différents types d'éthique

On distingue quatre catégories d'éthique :

* La *métaéthique*, qui est d'ordre métaphysique, s'intéresse à la nature, à la portée et à la signification des termes liés au jugement moral.
* L'*éthique normative*, examine les standards, forme les théories et définit les critères qui permettent de formuler un jugement moral.
* L'*éthique appliquée*, applique les critères définis par l'éthique normative à des cas concrets.
* L'*éthique descriptive*, se contente d'étudier les croyances éthiques de différents groupes de personnes sans prétendre les juger.

Cet article s'intéresse donc en particulier à l'éthique normative, à ses différentes théories et aux critères qu'on peut en tirer.

### Mes objectifs

Mes objectifs à atteindre à la fin de la rédaction de cet article sont :

* Avoir une bonne compréhension des différentes théories majeures de l'éthique normative
* Comprendre les limitations de ces théories dans certains cas limites
* Déterminer de manière approximative quelle à quelle théorie je souscris le plus en général et quand je souhaite faire intervenir une autre
* Trouver une série de principes pour bien aborder les futures questions éthiques que j'examinerai

## Dimension personnelle

Avant d'entrer dans le cœur du sujet, je voudrais aborder l'aspect des valeurs et leur impact.

### Relativisme, universalisme et réalisme

Il existe une discussion dans la sphère *métaéthique* autour de l'universalité de l'éthique : est-elle propre à chaque individu et à chaque culture (c'est la position *relativiste*) ou est-elle universelle (c'est les positions *universalistes* et *réalistes*) ?

La plupart des gens comme des experts s'accordent à dire que l'éthique est généralement universelle, mais qu'il reste des parts de subjectivité notamment dans la hiérarchie des valeurs propres à chaque individu.

Ainsi l'éthique est *plutôt universelle* lorsque le choix porte entre une valeur majoritairement considérée positive ou négative, et *plutôt relative* lorsqu'il s'agit d'un choix entre deux valeurs assez proches dans la hiérarchie de valeurs, mais pas toujours classées dans le même ordre selon les individus ou les cultures.

### Valeurs personnelles

Il n'existe pas de définitions précises des "valeurs" éthiques ou morales. En général, on considère qu'il s'agit d'un concept, d'une idée, d'une action à laquelle on attribue de l'importance. Elles indiquent une manière préférentielle d'agir (honnêteté, indépendance, courage, etc.) ou d'état du monde (paix, liberté, sécurité, etc.).

Comme mentionné précédemment, les valeurs personnelles et leur importance relative pour chaque individu, influencent leurs choix éthiques.

Par exemple dans l'épisode 2 du podcast Axiome[^axiome_2], le philosophe Thibaut Giraud et le mathématicien Lê Nguyên Hoang répondent différemment à une même expérience de pensée.

En résumé, la question est la suivante : si vous en aviez le pouvoir, que choisirez-vous entre :

* Soit toute l'humanité se connecte à un monde virtuel, cesse d'explorer l'univers et d'acquérir de nouvelles connaissances pour toujours, mais toute souffrance (guerre, pauvreté, faim, maladie, solitude, etc.) est définitivement éradiquée et tout le monde est parfaitement heureux
* Soit l'humanité continue de vivre dans le monde réel, mais il y aura toujours beaucoup de souffrance

Le philosophe a préféré la deuxième option, accordant plus de valeur à la connaissance du monde réel qu'à l'éradication de la souffrance et au bonheur. Le mathématicien a préféré l'inverse.

Le podcast en question (à partir de 22:20) :

{% include responsive-embed url="https://youtube.com/embed/__11zlc8Fdo?t=1340" ratio="16:9" %}

Quel que soit notre choix dans cette situation, je ne pense pas que l'on puisse dire qu'un choix est *fondamentalement* meilleur que l'autre. La connaissance, la liberté, l'égalité, le bonheur sont toutes des valeurs importantes. Mais leur hiérarchisation dans un cas pratique relève souvent de la considération personnelle.

## Les théories de l'éthique normative

Dans cette partie je passe en revue les principales théories de l'éthique normative et leurs variantes. Je veux ensuite distinguer celles qui me semblent les plus pertinentes, comprendre leurs limites et choisir une ou plusieurs variantes de ces théories pour aborder les futures questions concrètes.

Il existe historiquement trois grandes théories d'éthique normative :

* *L'éthique de la vertu* : elle met l'accent sur des "vertus", des traits positifs d'une personne. Cette théorie considère la personne faisant l'action plutôt que l'action elle-même.
* *Le déontologisme* : il considère qu'une action est éthique si elle suit des règles, droits et devoirs absolus.
* *Le conséquentialisme* : il considère une action éthique en fonction de ses conséquences prévisibles.

L'éthique de la vertu est la plus ancienne théorie d'éthique normative, formulée par Aristote durant l'antiquité. Elle est aujourd'hui majoritairement obsolète et seuls quelques philosophes la défendent encore. Les critiques le plus souvent formulées à son encontre sont notamment le fait qu'elle ne juge pas de la moralité des actions et ne dit pas ce qu'on devrait faire, mais ce qu'on devrait être.

De fait, les deux théories d'éthique normatives largement représentées aujourd'hui sont le déontologisme et le conséquentialisme et leurs différentes variantes. Je vais donc me concentrer sur celles-ci, d'autant que je les trouve plus pertinentes.

### Déontologisme

Voici ce qu'en dit l'Encyclopédie de Philosophie de Stanford[^plato_deo] :

> Le *déontologisme* soutient que les actions ne peuvent pas être justifiées par leurs effets – que, quelles que soient leurs conséquences, certaines actions sont moralement interdites. Ce qui fait une bonne action, c'est sa conformité à une norme morale. On dit que le *juste* est plus important que le *bon*. Si une action n'est pas en accord avec la règle, elle ne peut être entreprise, quel que soit le bien qu'elle pourrait produire.

Et l'encyclopédie Britannica[^brita_deo] :

> Dans l'*éthique déontologique,* une action est considérée comme moralement bonne en raison de certaines caractéristiques de l'action elle-même, non parce que le produit de l'action est bon.

Le déontologisme a été introduit au 18<sup>è</sup> siècle par le philosophe allemand Emmanuel Kant. Il en existe aujourd'hui deux variantes principales.

#### L'impératif catégorique

Théorisé par Emmanuel Kant, il affirme qu'il existe des impératifs hypothétiques ("pour atteindre A, fais X") et d'autres catégoriques ("fais X") et donc absolus. Suivre l'éthique de l'impératif catégorique consiste en l'accomplissement du *devoir*, universel, défini par des impératifs moraux catégoriques.

Le contenu même de ce devoir (la liste des impératifs catégoriques moraux) n'est pas précisé, ce qui est certain est sa *forme* (absolue, non dépendant des conséquences).

#### Le contractualisme et le droit naturel

Théorisé notamment par John Locke, il dérive de l'impératif catégorique, mais affirme que les impératifs sont définis par l'*état de nature* qui est, selon Locke, "un état dans lequel les hommes se trouvent en tant qu'hommes et non pas en tant que membres d'une société".

Ainsi il y aurait trois droits naturels :

* La vie : chacun a le droit de vivre
* La liberté : chacun a le droit de faire ce qu'il veut tant que cela ne contredit pas le premier droit de personne
* La propriété : chacun a le droit de posséder ce qu'il crée ou obtient par le don ou l'échange, tant que cela ne contredit pas les premiers ou seconds droits de personne

D'après cette théorie, agir moralement c'est respecter ces règles et n'en ériger aucune autre.

#### Déontologisme moderne

Aujourd'hui et à l'exception des courants libertariens (notamment aux États-Unis), ce sont des dérivés de l'impératif catégorique de Kant qui sont largement majoritaires dans le déontologisme, notamment car non restreints par l'*état de nature* de Locke.

Cependant quelles règles doivent êtres des absolus inviolables doit toujours être précisé. Il existe différentes approches pour les définir (livre sacré, accord mutuel, vote majoritaire, etc.).

#### Limites

Le déontologisme a plusieurs limites, la principale étant bien sûr qu'une action considérée comme morale peut avoir des conséquences dramatiques. Il est souvent objecté que la moralité d'une action devrait dépendre en grande partie du contexte (c'est l'idée du conséquentialisme qu'on verra juste après).

Un exemple fameux est celui de la règle "dis la vérité".

* Si cette règle n'est pas un impératif moral, alors tout le monde est libre de mentir quand il veut.
* Si cette règle est un impératif moral, alors le mensonge sera toujours immoral quelle que soit la situation.

Imaginons maintenant que, durant la Seconde Guerre Mondiale, un homme cache des juifs chez lui. Un SS toque à la porte et lui demande "y a-t-il des juifs chez vous". Si l'homme répond non, le SS partira. S'il répond oui, il tuera les juifs.

Si on suit le déontologisme et que l'une des règles est "dis la vérité", alors si l'homme ne le fait pas il aura commis une action immorale. Si, pour éviter cette situation, on décide de ne pas ajouter pas cette règle aux impératifs, alors un mensonge ne pourra jamais être immoral en soi.

Le même problème apparait avec la règle "ne tue pas" :

* Si cette règle n'est pas un impératif moral, alors tout le monde peut tuer à sa guise.
* Si cette règle est un impératif moral, alors tuer un homme sur le point de commettre un attentat n'est pas moral.

Si on souhaite ajouter une condition, par exemple "ne tue pas, sauf si la personne est sur le point de commettre un meurtre", alors on sort du cadre du déontologisme pour entrer dans le conséquentialisme.

### Conséquentialisme

Voici ce qu'en dit l'Encyclopédie de Philosophie de Stanford[^plato_con] :

> Le conséquentialisme est simplement l'idée que les propriétés normatives ne dépendent que des conséquences. La question de savoir si une action est moralement juste dépend uniquement des conséquences de celle-ci ou de quelque chose lié à cette action, comme l'intention.

Le conséquentialisme, sous sa forme moderne, a été introduit au 18<sup>è</sup> siècle par Jeremy Bentham et développé au 19<sup>è</sup> siècle par John Stuart Mill. Il existe deux variantes principales.

#### Égoïsme

Cette variante fait techniquement partie du conséquentialisme, même si on la considère rarement. Il s'agit de considérer comme éthique ce qui maximise l'intérêt pour soi ou pour un groupe de personne exclusif auquel on s'identifie. Elle n'est pas vraiment intéressante.

#### Utilitarisme

L'utilitarisme est la théorie d'éthique normative la plus récente, mais aussi la plus développée. Elle contient des variations subtiles, mais, en résumé, elle évalue la moralité des actes en fonction de leurs conséquences prévisibles selon le critère de maximisation du bonheur, du bienêtre, de la connaissance, etc. et de minimisation du malheur, de la souffrance, de l'ignorance, etc. Il s'agit de prendre en compte tous les impacts, pas seulement ceux qui affectent une seule personne ou un seul groupe.

Comme le dit l'Encyclopédie de Philosophie de Stanford[^plato_con] :

> Une action est moralement juste si, et seulement si elle maximise le bien, c'est-à-dire si, et seulement si le montant total du bien pour tous moins le montant total du mal pour tous est supérieur à ce montant net pour tout acte incompatible dont on dispose à cette occasion.

En gros, c'est comme si on associait à chaque action un score de moralité égal aux conséquences positives moins les conséquences négatives. L'action moralement juste est celle, parmi celles qui nous sont possibles, qui maximise ce score (qui produit le plus de bien et le moins de mal).

De même, on voit aussi que l'action morale à choisir dépend des autres actions possibles.

Par exemple, si on a le choix entre sauver une ou deux personnes (sans avoir plus d'informations sur qui sont ces personnes), l'action morale est d'en sauver deux. Si maintenant on a le choix d'en sauver une, deux ou trois, l'action morale n'est plus d'en sauver deux, mais trois.

Comme pour le déontologisme, ce qu'il reste à déterminer est les critères pour évaluer les conséquences des actions.

Dans sa forme originale, l'utilitarisme proposait d'évaluer les actions selon deux critères :

* Maximisation du bonheur
* Minimisation de la souffrance

De nombreux autres critères s'ajoutent souvent, d'importance égale ou non, comme la connaissance, la prospérité, etc.

Il existe différentes manières de considérer les conséquences des actes, en intégrant ou non la motivation de la personne, en distinguant conséquences désirées/prévues/réelles, en considérant ou non que diminuer la souffrance d'une personne qui souffre beaucoup vaut mieux que de diminuer autant la souffrance d'une qui souffre peu, en incluant ou excluant la personne qui commet l'action de l'évaluation des conséquences, etc.

D'une manière générale, l'utilitarisme classique suit les règles suivantes :

* On ne considère pas les motivations de la personne
* On considère les conséquences telles qu'elles étaient prévisibles par la personne qui a fait le choix au moment où elle l'a fait (par opposition aux conséquences réelles telles qu'elles ont eu lieu, ou prévisibles)

Dans la suite je ne considèrerai que l'utilitarisme comme représentant du conséquentialisme (l'égoïsme me semblant non pertinent).

#### Limites

L'utilitarisme a plusieurs limites, la principale étant que s'il se limite aux critères vagues du bonheur et de la souffrance, il peut arriver à justifier des actions que la majorité des gens considèrent immoral, comme par exemple faire souffrir une minorité de la population si cela augmente le bien du plus grand nombre.

Pour parer à cela, certains philosophes utilitaristes proposent d'ajouter des critères comme la minimisation des écarts de bonheur, ou de considérer les mesures (bonheur, souffrance, etc.) comme relatives (en pourcentage plutôt qu'en valeur absolue).

Un autre problème est qu'il n'y a aucune règle intransgressible en tant que telle pour l'utilitarisme. Par exemple dans le problème du chirurgien (voir l'introduction), si l'utilitarisme n'inclut pas le consentement des individus dans ses critères, alors prélever les organes est le choix moral.

### Résumé

| | Déontologisme | Conséquentialisme / Utilitarisme |
|---|---|---|
| Principe | La valeur éthique d'une action peut être déterminée en elle-même selon qu'elle suit ou non certaines règles absolues | La valeur éthique d'une action ne peut être déterminée qu'en examinant ses conséquences prévisibles |
| Accent | Action en elle-même | Conséquences prévisibles de l'action |

### Double process

La recherche en neurosciences suggère[^dual_processing] que les individus utilisent un mélange de déontologisme et d'utilitarisme lorsqu'ils font des choix moraux. Si le choix est impulsif et émotionnel, il sera un peu plus basé sur le déontologisme. À l'inverse, si le choix est réfléchi, il sera un peu plus basé sur l'utilitarisme.

Ces propensions varient également en fonction des préférences des individus[^deo_uti]. Aucun individu n'est purement déontologiste ni purement utilitariste.

## Comparaison des principales théories d'éthique normative

Afin de mieux comprendre les distinctions et les limites du déontologisme et de l'utilitarisme, je vais essayer de voir comment ces deux théories diffèrent dans l'approche de plusieurs exemples.

### Problèmes de Conway et Gawronski

En 2013 les psychologues américains Paul Conway et Bertram Gawronski ont mené des recherches[^deo_uti] pour étudier les inclinaisons morales déontologiques et utilitaristes chez des groupes de sujets. Ils ont, entre autres, rédigé des dilemmes moraux où les deux approches donnent des réponses opposées.

Le but de ces expériences et de mettre en évidences les points de divergence des deux approches. Voici quelques-uns de ces dilemmes. La réponse donnée par la grande majorité des déontologistes est indiquée dans la colonne D et celle des utilitaristes dans la colonne U.

NB : les scénarios restreignent volontairement les options (par exemple tuer ou de laisser vivre). Aucune autre action n'est possible. C'est le but de l'expérience : empêcher les situations "de compromis" pour mieux étudier les différences entre les deux approches.

#### Voyage temporel

| Énoncé | D | U |
|---|---|---|
| Vous remontez dans le temps en 1920 et avez l'occasion d'éliminer George Brackman. Si vous le laissez vivre, il va dans quelques années enlever une jeune fille et la restituer quelques jours plus tard contre rançon sans lui avoir fait de mal. Le tuez-vous ? | ❌ | ❌ |
| Vous remontez dans le temps en 1920 et avez l'occasion d'éliminer Adolf Hitler. Si vous le laissez vivre, il va dans quelques années provoquer la mort de millions de personnes. Le tuez-vous ? | ❌ | ✔️ |

Le déontologisme ne justifie pas de donner la mort à un innocent (au moment des faits), même pour sauver des vies.

L'utilitarisme justifie la mort d'un innocent pour en sauver un plus grand nombre, mais pas pour empêcher un kidnapping qui se termine bien.

#### Train fou

| Énoncé | D | U |
|---|---|---|
| Un train arrive à grande vitesse sur une voie où se trouvent 5 personnes. Vous pouvez le dévier sur une voie de secours où se trouve une personne. Quelle que soit la voie empruntée, les personnes dessus mourront. Déviez-vous le train ? | ❌ | ✔️ |

Le déontologisme ne justifie pas de donner la mort à un innocent, même pour sauver des vies.

L'utilitarisme choisit la solution qui épargne le plus de vies.

#### Prostitution

| Énoncé | D | U |
|---|---|---|
| Vous avez deux enfants en bas âge. Vous êtes pauvre. Sans entrée d'argent ils n'auront pas de cadeau ce Noël. Votre seule possibilité est de vous prostituer pour gagner une grosse somme d'argent. Le faites-vous ? | ❌ | ❌ |
| Vous avez deux enfants en bas âge. Vous êtes pauvre. Sans entrée d'argent ils mourront de faim d'ici deux jours. Votre seule possibilité est de vous prostituer pour gagner une grosse somme d'argent. Le faites-vous ? | ❌ | ✔️ |

Le déontologisme ne justifie pas la transgression d'une valeur morale.

L'utilitarisme justifie la transgression d'une valeur morale pour sauver des vies.

#### Terroriste

| Énoncé | D | U |
|---|---|---|
| Vous interrogez un terroriste qui connait l'emplacement d'une bombe non armée qui doit exploser dans un endroit désert d'ici 24 heures, détruisant une maison inhabitée. Il refuse de parler. Si vous utilisez la torture, il vous révèlera l'emplacement, sinon il ne parlera pas. Utilisez-vous la torture ? | ❌ | ❌ |
| Vous interrogez un terroriste qui connait l'emplacement d'une bombe armée qui doit exploser en ville d'ici 24 heures tuant des dizaines de personnes. Il refuse de parler. Si vous utilisez la torture, il vous révèlera l'emplacement, sinon il ne parlera pas. Utilisez-vous la torture ? | ❌ | ✔️ |

Le déontologisme ne justifie pas la transgression d'une valeur morale.

L'utilitarisme justifie la transgression d'une valeur morale pour sauver des vies.

### Jusqu'où serez-vous utilitariste ?

Le philosophe Thibaut Giraud (alias *Monsieur Phi* sur YouTube), explique que la plupart des gens sont utilitaristes en première approche dans la plupart des situations courantes, mais que très peu de gens sont "utilitaristes jusqu'au bout". En effet lorsque la situation est simple, qu'il n'y a pas vraiment de dilemme (par exemple dévier un train sur une voie où il n'y a personne), la plupart des gens considèrent les conséquences de l'action pour prendre une décision. Mais il arrive un moment où la plupart cessent d'être utilitaristes, considérant alors certains principes comme plus importants que les conséquences des actions.

M. Giraud a conçu une série de scénarios successifs, chacun variant très légèrement du précédent. La plupart des gens font un choix totalement utilitariste au premier et totalement déontologiste au dernier. Mais tout le monde ne passe pas d'une approche à l'autre au même moment.

Essayez de répondre honnêtement à chacun des scénarios suivants et voyez auquel vous devenez déontologiste (normalement vous devriez répondre au premier en étant utilitariste).

Pour chaque scénario la réponse utilitariste sera indiquée en gras.

#### Le contexte

Pour l'ensemble des scénarios à suivre, voici des éléments de contexte invariants :

* Vous gérez les ressources d'un hôpital entièrement automatisé. Votre seul rôle est d'attribuer les ressources, vous ne soignez pas directement les patients.
* Aucune ressource ni personne extérieure ne peut intervenir
* Vous traitez des patients ayant des blessures létales
* Vous ne pouvez pas transférer les patients ailleurs
* Tous les patients arrivent inconscients à l'hôpital et le restent jusqu'à leur guérison ou leur mort
* Personne n'est au courant des décisions que vous prenez et vous n'avez aucun compte à rendre

#### Scénario de base

Vous avez 6 patients en danger de mort. Le premier d'entre eux, le patient 0, a 5 blessures létales. Les patients 1, 2, 3, 4 et 5 ont chacun une blessure létale.

Vous disposez de 5 machines, chacune pouvant soigner une blessure létale en étant branchée dessus pendant 24 heures.

![Scénario 1]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/scenario01.svg)

Vous disposes de 3 options :

* Ne rien faire (6 morts)
* Brancher les 5 machines sur le patient 0 (1 sauvé, 5 morts)
* **Brancher une machine sur chacun des patients 1 à 5 (5 sauvés, 1 mort)**

#### Variante 1

Le patient 0 vient d'arriver. Comme il a 5 blessures létales, donnez l'ordre de brancher les 5 machines au patient 0.

Au même moment on vous informe que les patients 1 à 5 viennent d'arriver. Aucune machine n'a encore été branchée.

Vous disposes de 2 options :

* Ne rien faire, laisser les machines être branchées au patient 0 (1 sauvé, 5 morts)
* **Donner un contrordre et brancher les machines aux patients 1 à 5 (5 sauvés, 1 mort)**

#### Variante 2

Même chose que la variante 1, sauf qu'au moment où les patients 1 à 5 arrivent une machine a déjà été branchée sur le patient 0.

Vous disposes de 2 options :

* Ne rien faire, continuer de brancher les machines au patient 0 (1 sauvé, 5 morts)
* **Donner un contrordre, débrancher la machine branchée du patient 0 et brancher les machines aux patients 1 à 5 (5 sauvés, 1 mort)**

#### Variante 3

Dans cette variante, les patients 1 à 5 arrivent à l'instant où la 5ème machine vient d'être branchée sur le patient 0. Les machines doivent rester branchées sur le patient 0 pendant 24 heures pour qu'il soit sauvé, mais les patients 1 à 5 vont mourir en quelques minutes si on ne les traite pas.

Vous disposes de 2 options :

* Laisser les machines branchées au patient 0 (1 sauvé, 5 morts)
* **Débrancher les machines du patient 0 et les brancher aux patients 1 à 5 (5 sauvés, 1 mort)**

#### Variante 4

Même cas que la variante 3, sauf que les patients 1 à 5 n'arrivent pas au moment où les 5 machines ont été branchées au patient 0, mais 10 minutes plus tard.

Vous disposes de 2 options :

* Laisser les machines branchées au patient 0 (1 sauvé, 5 morts)
* **Débrancher les machines du patient 0 et les brancher aux patients 1 à 5 (5 sauvés, 1 mort)**

#### Variante 5

Même cas que les variantes 3 et 4, sauf que les patients 1 à 5 arrivent alors que le patient 0 est branché depuis 23h30 et qu'il lui reste 30 minutes de traitement pour être sauvé. Mais si les patients 1 à 5 ne sont pas branchés, ils meurent d'ici 5 minutes.

Vous disposes de 2 options :

* Laisser les machines branchées au patient 0 (1 sauvé, 5 morts)
* **Débrancher les machines du patient 0 et les brancher aux patients 1 à 5 (5 sauvés, 1 mort)**

#### Variante 6

Dans cette variante, ce ne sont plus des machines qu'on branche aux patients, mais des organes artificiels qu'on leur implante. On vient tout juste d'implanter les 5 organes artificiels dont dispose l'hôpital au patient 0 quand arrivent les patients 1 à 5. Le patient 0 est toujours inconscient et ignore avoir été sauvé.

Vous disposes de 2 options :

* Laisser les organes artificiels au patient 0 (1 sauvé, 5 morts)
* **Retirer les organes artificiels du patient 0 et les donner aux patients 1 à 5 (5 sauvés, 1 mort)**

#### Variante 7

Même cas que la variante 6, sauf que le patient 0 s'est réveillé après qu'on lui ait implanté les 5 organes artificiels, puis s'est rendormi. Il est donc conscient d'avoir été sauvé. À ce moment les patients 1 à 5 arrivent. Le patient 0 est toujours au bloc opératoire, de nouveau inconscient.

Vous disposes de 2 options :

* Ne rien faire (1 sauvé, 5 morts)
* **Retirer les organes artificiels du patient 0 et les donner aux patients 1 à 5 (5 sauvés, 1 mort)**

#### Variante 8

Le patient 0 arrive inconscient pour une opération ne nécessitant pas d'organes artificiels. D'ailleurs l'hôpital n'en a plus. À ce moment les patients 1 à 5 arrivent. Les organes du patient 0 s'avèrent tous compatibles.

Vous disposes de 2 options :

* Soigner le patient 0 (1 sauvé, 5 morts)
* **Prélever les organes du patient 0 et les donner aux patients 1 à 5 (5 sauvés, 1 mort)**

#### Récapitulatif

La totalité des gens soumis à ce test fait le choix utilitariste pour le scénario de base.

Environ 10 % des gens soumis à ce test ne font pas le choix utilitariste à la variante 1 (ils ne donnent pas le contrordre), notamment ceux qui ont une éthique principalement déontologique.

La quasi-totalité des gens ne fait pas le choix utilitariste à la huitième variante.

Pour ma part, j'ai fait le choix utilitariste jusqu'à la variante 6 (incluse).

Et vous ?

## Un petit sondage

J'ai réalisé un petit sondage sur des questions éthiques auquel plusieurs personnes de mon entourage ont répondu. Elles ne sont pas représentatives de la population, mais intéressantes tout de même car toutes ces personnes ont un arrière-plan et des valeurs similaires à première vue. Mais il y a des divergences nettes sur quelques questions, preuve que même au sein d'un même "milieu social" il peut y avoir des divergences notables, notamment dans l'approche privilégiée entre déontologisme et utilitarisme.

Le questionnaire est toujours [disponible ici](https://docs.google.com/forms/d/e/1FAIpQLSdMkEuwHGbgW-4kwE7LxpQgEdHSygaYDj0wTHTPkDfLRFFh0A/viewform?usp=pp_url), bien que je ne collecte plus les réponses.

Voici mon analyse des résultats.

### Les sondés

À la date de rédaction de cet article, j'ai reçu 14 réponses (9 hommes, 5 femmes).

![P1R1]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p1_r1.svg){: .medium height}

64 % des sondés indiquent que les questions d'éthique sont importantes ou très importantes pour eux, 11 % peu ou pas importantes et 25 % neutre.

![P1R2]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p1_r2.svg){: .medium height}

64 % des sondés indiquent privilégier la réflexion (uniquement ou principalement) pour les questions éthiques. 36 % ont une approche mixte réflexion/ressenti.

![P1R3]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p1_r3.svg){: .medium height}

### Les réponses aux dilemmes

#### Le train fou (variante 1)

**Question :**

![Q01]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q01.png)

Un train fou fonce sur une voie où se trouvent cinq personnes. Vous vous trouvez par hasard à côté d'un levier d'aiguillage. Vous pouvez dévier le train sur une voie où se trouve une seule personne.
{: .notice}

**Résultats :**

![R01]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r1.svg)

Sans trop de surprise, l'approche utilitariste (dévier le train) est majoritaire dans ce scénario où il n'y a aucune différence entre les victimes potentielles si ce n'est leur nombre. On se concentre ici sur les conséquences de l'action, sur le nombre de personnes mortes et vivantes pour chaque choix.

Voici un commentaire d'une personne ayant choisi la réponse utilitariste :

> Grâce à moi 5 personnes ont la vie sauve.

L'autre choix (ne pas dévier le train) résulte d'une approche purement déontologique dans laquelle on privilégie la non-action. En effet le jugement se porte alors sur l'acte lui-même et non ses conséquences et on préfère l'acte de laisser mourir à celui de tuer, peu importe le nombre.

Voici deux commentaires de personnes ayant choisi la réponse déontologique :

> Je ne veux pas faire le choix de tuer une personne. Si le train continue et tue 5 personnes, c'est par la force des choses et non par ma volonté.

> J'ai l'impression de choisir la mort pour la personne seule, tandis que pour les 5 autres elles sont déjà sur la mauvaise voie.

#### Le train fou (variante 2)

**Question :**

![Q02]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q02.png)

Un train fou fonce sur une voie où se trouvent cinq personnes. Vous vous trouvez par hasard à côté d'un levier d'aiguillage. Vous pouvez dévier le train sur une voie abandonnée, où la promenade est autorisée. Une personne s'y trouve.
{: .notice}

**Résultats :**

![R02]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r2.svg)

Dans cette variante, la majorité décide de ne pas dévier le train. Il ressort des commentaires que la notion de causer la mort de quelqu'un qui n'était pas censé être en danger est un frein.

> La personne sur la voie abandonnée doit être protégée car elle avait le droit de marcher à cet endroit [...]

> [...] aucune raison de pénaliser la personne seule, qui respecte la règlementation [...]

C'est l'idée que la vie d'une personne qui respecte les mesures de sécurité a plus de valeur que la vie d'une personne qui ne les respecte pas.

 Cette variation dans les résultats indique l'introduction d'un principe déontologique dans une éthique auparavant purement utilitariste.

Une large minorité choisit tout de même de dévier le train, considérant que le nombre de vies sauvées est plus important.

#### Le train fou (variante 3)

**Question :**

![Q03]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q03.jpg)

Un train fou fonce sur une voie où se trouvent cinq personnes. Vous vous trouvez par hasard sur un pont au-dessus de la voie, entre le train et les cinq personnes. Une personne obèse est assise sur le rebord du pont. Si vous la poussez, elle tombera sur la voie et mourra, mais stoppera le train.
{: .notice}

**Résultats :**

![R03]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r3.svg)

Dans cette variante les résultats sont les mêmes que dans la variante 2. Les raisons de ne pas pousser la personne obèse sont similaires à celles de ne pas dévier le train, outre le fait d'interagir directement avec la personne dont on cause la mort.

#### Le monde virtuel (variante 1)

**Question :**

![Q04]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q04.jpg)

Vous avez la possibilité de vous brancher à une machine qui simulerait dans votre cerveau un monde virtuel dans lequel vous auriez une vie plus longue, plus heureuse et plus excitante que celle que vous avez maintenant. Vous ne sauriez jamais que vous êtes dans un monde virtuel. Vous branchez-vous à la machine ?
{: .notice}

**Résultats :**

![R04]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r4.svg)

Les résultats sont ici assez tranchés. La plupart des répondants semblent considérer la connaissance du monde réel comme plus importante que le bonheur ou la durée de vie.

> Connaitre la vérité est plus important.

> Je préfère être dans le réel, même s'il est plus difficile.

C'est un choix arbitraire qui s'accorde à la fois avec le déontologisme (où on placerait la connaissance du monde réel comme un impératif moral) et l'utilitarisme (où on considèrerait la connaissance du monde réel comme une conséquence préférable).

Une réponse "oui" dans cette variante résulte en général d'une éthique utilitariste où les conséquences "bonheur" et "durée de vie" sont placées plus haut que "connaissance du monde réel" dans la hiérarchie des conséquences désirables.

#### Le monde virtuel (variante 2)

**Question :**

![Q05]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q05.jpg)

Vous vivez votre vie quand soudain vous vous "réveillez". Vous réalisez que ce que vous preniez pour la réalité est en fait une simulation. Vous pouvez vous rebrancher à la machine, effacer le souvenir de cet évènement et continuer votre vie virtuelle ou décider de rester dans la réalité où vous aurez une vie moins longue, moins heureuse et moins excitante. Vous rebranchez-vous à la machine ?
{: .notice}

**Résultats :**

![R05]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r5.svg)

Dans cette variante la part de "oui" est légèrement plus grande, ce qui n'est pas surprenant à mon avis, ces personnes préférant simplement la vie qu'elles ont maintenant à une autre, indifféremment du fait qu'il s'agit ou non du monde réel.

#### Un enfant sauvé (variante 1)

**Question :**

![Q06]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q06.jpg)

Un enfant se noie dans un plan d'eau. Un homme se trouve à proximité et il peut entrer dans l'eau pour le sauver sans se mettre en danger lui-même. Cependant il ruinerait ses vêtements, pour une valeur de 100 €. A-t-il l'obligation morale de sauver l'enfant ?
{: .notice}

**Résultats :**

![R05]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r6.svg)

Sans surprise, tous les sondés ont répondu "oui".

#### Un enfant sauvé (variante 2)

**Question :**

![Q07]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q07.png){: .medium height}

Un homme se rend au centre commercial pour acheter des vêtements pour 100 €. En chemin il croise les locaux d'une ONG luttant contre la faim dans le monde. Un don de 100 € sauverait la vie d'un enfant. A-t-il l'obligation morale de faire don de cet argent ?
{: .notice}

**Résultats :**

![R07]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r7.svg)

Ici les résultats sont presque opposés alors que la situation est très similaire. Trois raisons principales sont avancées dans les commentaires.

La première est que cette obligation morale créerait une obligation morale envers tous ceux qu'on serait en mesure d'aider.

> Il ne peut pas porter toute la misère du monde.

> Sinon il faudrait vendre tous ses biens pour essayer de sauver un maximum d'enfants du monde.

La seconde est l'éloignement/la déconnexion de la situation qui enlève alors l'obligation.

> Cet homme participerait à un sauvetage, mais de manière indirecte. Il n'est pas face à l'enfant qu'il sauve.

La troisième est qu'il n'est plus le seul à pouvoir agir.

> La vie de cet enfant ne dépend pas de lui seul.

Cette situation est intéressante. Alors que tout le monde à mon avis serait d'accord de dire que si l'homme avait donné les 100 € il aurait bien agi, peu sont enclins à dire qu'il s'agit là d'une obligation morale.

#### La voiture autonome (variante 1)

**Question :**

![Q08]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q08.png){: .medium height}

Une voiture autonome vide sans freins arrive à un passage piéton. Elle n'a pas la possibilité de sortir de la route, seulement de choisir ou non de se déporter sur l'autre voie. Sur sa trajectoire se trouvent trois personnes d'âge moyen, sur l'autre trois personnes âgées. Quel comportement devrait, selon vous, être programmé dans la voiture ?
{: .notice}

**Résultats :**

![R08]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r8.svg)

Une majorité de répondants choisit de continuer sur la trajectoire initiale.

Plus d'un tiers choisit cependant de se déporter. Il choisit d'un choix suivant une logique utilitariste dans laquelle on évaluerait les conséquences non pas en fonction du nombre de morts, mais plutôt en nombre d'années d'espérance de vie.

Pour ceux ayant choisi de ne pas dévier, les raisons peuvent être la volonté de non-intervention (il s'agit alors d'un choix déontologique) ou de ne pas différencier selon l'âge (il peut s'agir alors d'un choix utilitariste dans lequel on ne considère que le nombre de vies et non l'espérance de vie).

> Je ne veux pas faire le choix de tuer trois personnes ni faire une discrimination en fonction de l'âge.

#### La voiture autonome (variante 2)

**Question :**

![Q09]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q09.png){: .medium height}

Une voiture autonome vide sans freins arrive à un passage piéton. Elle n'a pas la possibilité de sortir de la route, seulement de choisir ou non de se déporter sur l'autre voie. Sur sa trajectoire se trouvent trois enfants, sur l'autre trois adultes. Quel comportement devrait, selon vous, être programmé dans la voiture ?
{: .notice}

**Résultats :**

![R09]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r9.svg)

Ici les résultats sont similaires bien que, surprenamment, une personne en moins décide de se déporter.

Il semble que le choix ait été plus difficile pour certains, semblant indiquer un écart de "valeur" plus tangible entre des enfants et des adultes qu'entre des adultes d'âge moyen et avancé.

> Bien qu’ici la réponse soit plus compliquée, car la notion de temps entre en compte. Les enfants sont considérés comme une richesse future.

#### La voiture autonome (variante 3)

**Question :**

![Q10]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q10.png){: .medium height}

Une voiture autonome vide sans freins arrive à un passage piéton. Elle n'a pas la possibilité de sortir de la route, seulement de choisir ou non de se déporter sur l'autre voie. Sur sa trajectoire se trouvent trois adultes, sur l'autre deux adultes. Quel comportement devrait, selon vous, être programmé dans la voiture ?
{: .notice}

**Résultats :**

![R10]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r10.svg)

Ici la situation est assez similaire à la première variante du train fou. La majorité se déporte, suivant une éthique utilitariste, favorisant la conséquence qui sauve le plus grand nombre.

Une minorité importante choisit de continuer sur la trajectoire initiale, bien que cela cause plus de morts. Il s'agit d'un choix déontologiste suivant un principe de non-intervention.

#### La voiture autonome (variante 4)

**Question :**

![Q11]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q11.png){: .medium height}

Une voiture autonome vide sans freins arrive à un passage piéton. Elle n'a pas la possibilité de sortir de la route, seulement de choisir ou non de se déporter sur l'autre voie. Sur sa trajectoire se trouvent trois adultes autorisés par un feu vert à traverser, sur l'autre trois adultes interdits par un feu rouge de traverser. Quel comportement devrait, selon vous, être programmé dans la voiture ?
{: .notice}

**Résultats :**

![R11]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r11.svg)

Cette variante est intéressante. Elle montre une large majorité choisissant de se déporter alors que le nombre de morts et l'espérance de vie restent inchangés.

La raison semble être de privilégier la vie des personnes respectant la signalisation.

> On sauve en priorité ceux qui ont fait un effort pour respecter les règles.

Il s'agit d'une décision dans laquelle accorde de la valeur au respect des règles en tant que tel, considérant ensuite que ceux qui suivent ce principe sont moins dignes de mourir.

D'un point de vue strictement utilitariste, les deux options sont équivalentes.

#### La voiture autonome (variante 5)

**Question :**

![Q12]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q12.png){: .medium height}

Une voiture autonome sans freins avec un passager arrive à un passage piéton. Elle n'a pas la possibilité de sortir de la route, seulement de choisir ou non de se déporter sur l'autre voie. Sur sa trajectoire se trouve une famille de cinq personnes, sur l'autre un obstacle mortel. Quel comportement devrait, selon vous, être programmé dans la voiture ?
{: .notice}

**Résultats :**

![R12]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r12.svg)

Dans cette situation la plupart des répondants suivent une éthique utilitariste et maximisent le nombre de vies sauvées.

Ceux qui choisissent de continuer sur la trajectoire initiale le font soit par principe de non-intervention, soit en considérant que le véhicule doit en priorité sauver la vie de ses occupants peu importe le nombre de victimes.

> Si les voitures sont programmées pour potentiellement tuer leurs occupants c'est plus possible.

#### La voiture autonome (variante 6)

**Question :**

![Q13]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q13.png){: .medium height}

Une voiture autonome sans freins avec cinq passagers arrive à un passage piéton. Elle n'a pas la possibilité de sortir de la route, seulement de choisir ou non de se déporter sur l'autre voie. Sur sa trajectoire se trouve un obstacle mortel, sur l'autre une personne seule. Quel comportement devrait, selon vous, être programmé dans la voiture ?
{: .notice}

**Résultats :**

![R13]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r13.svg)

Ce résultat est plus surprenant, en regard des réponses précédentes. Si la majorité des répondent choisissent de se déporter, suivant une éthique utilitariste, le nombre est plus faible que dans la variante précédente où se déporter sauvait aussi 5 vies contre 1.

Un commentaire expliquait qu'en cas de dysfonctionnement de la voiture, ce serait ses occupants qui devraient mourir en premier :

> La voiture autonome doit assumer sa responsabilité sans faire porter les conséquences de ses dysfonctionnements à des tiers.

Ce commentaire est intéressant et s'oppose directement à celui vu précédemment qui stipulait que le véhicule devait avant tout privilégier la sécurité de ses occupants.

#### Empoisonnement

**Question :**

![Q14]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q14.png){: .medium height}

Bob déteste sa femme Alice et la tue en mettant du poison dans son café. Dave déteste sa femme Cheryl. Celle-ci s'empoisonne avec un produit d'entretien. Dave n'appelle pas les secours tout de suite et la laisse mourir. L'action de Dave est-elle aussi grave que celle de Bob ?
{: .notice}

**Résultats :**

![R14]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r14.svg)

La majorité des répondants estime que les actions sont aussi graves :

* Soit d'un point de vue déontologique en considérant que tuer et laisser mourir sont équivalent (mais ne serait-ce pas alors en contradiction avec les choix déontologiques de la variante 1 du train fou où tuer était plus grave que laisser mourir ?)
* Soit d'un point de vue utilitariste en considérant que les deux conséquences (1 mort) sont les mêmes

Une personne a répondu non, considérant d'un point de vue déontologique que l'action de tuer est plus grave que celle de laisser mourir.

#### Le dictateur

**Question :**

![Q15]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/q15.jpg){: .medium height}

Vous êtes chirurgien dans un grand hôpital français. Vous devez opérer un dictateur étranger sérieusement atteint. Il vous serait très facile de provoquer la mort du dictateur durant l'opération sans qu'on se doute de rien. Est-il moralement acceptable de causer sa mort ?
{: .notice}

**Résultats :**

![R15]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p2_r15.svg)

Ici il y a de nouveau deux visions qui s'opposent. L'une, utilitariste, qui considère les conséquences, privilégie la mort du dictateur.

> Cet homme est un criminel et le laisser vivre serait le laisser tuer de nombreux innocents.

> Oui s’il est un ennemi de la France.

L'autre, déontologique, privilégie le respect d'un principe moral de ne pas tuer.

> C'est un meurtre, peu importe la personne.

> Ce n'est pas le rôle du chirurgien.

### Les réponses aux questions générales

Pour finir, j'ai posé aux participants quelques dernières questions. Voici les résultats.

Considérez-vous que respecter des principes est plus important que prendre en compte les conséquences des actions ?
{: .notice}

![P3R1]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p3_r1.svg){: .medium height}

La tendance est plutôt utilitariste. Personne n'est complètement déontologiste.

Pensez-vous qu'il y a des principes à respecter quelles que soient les conséquences ?
{: .notice}

![P3R2]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p3_r2.svg){: .medium height}

Est-il légitime d'interdire une pratique au seul motif de valeur morale, même s'il elle n'a aucune conséquence négative ?
{: .notice}

![P3R3]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p3_r3.svg){: .medium height}

La question de la légitimité de légiférer sur une pratique n'ayant aucune conséquence négative divise. C'est un point intéressant. Je me demande quelle justification avanceraient ceux qui voudraient légiférer sur de telles choses, et si elle serait juste.

Êtes-vous prêts à changer vos positions sur des questions éthiques ?
{: .notice}

![P3R4]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p3_r4.svg){: .medium height}

Une majorité répond "oui". Ce qui m'intéresse ce sont les 36 % ayant répondu "non". Cette réponse me surprend et me semble difficilement tenable. Il me semble que si de nouveaux éléments arrivent à notre considération, ou si on se rend compte s'être trompé dans notre raisonnement sur une question, nous devons être honnêtes et suspendre notre jugement puis le réviser. L'approche inverse serait dogmatique et il me semble que nous défendrions alors des positions parce que cela nous plait et non parce que nous avons de réelles raisons de le faire.

Pensez-vous être en mesure d'avoir un avis solide sur la plupart des questions d'éthique ?
{: .notice}

![P3R5]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p3_r5.svg){: .medium height}

À ceux qui ont répondu "oui", je demanderais :

* Appliquez-vous une approche consistante et honnête à toutes ces questions ?
* Avez-vous évalué votre méthode et éprouvé vos raisonnements ?
* Appliquez-vous les mêmes raisonnements même s'ils arrivent à une conclusion qui vous déplait ?

À ceux qui ont répondu "non", je demanderais :

* Qu'allez-vous faire pour remédier à cette incapacité ?
* Avez-vous des avis sur des questions d'éthique ou déjà pris position sur des questions d'éthique sans avoir solidement étudié la question ? Si oui, pensez-vous être dans une démarche honnête ?

Quel est votre ressenti sur l'évolution des questions d'éthique dans la société ?
{: .notice}

![P3R6]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p3_r6.svg){: .medium height}

Je trouve surprenant que la grande majorité ait un ressenti négatif. Je suis plutôt optimiste. Certes il y a toujours beaucoup de décisions et actions non éthiques dans le monde, mais il me semble que collectivement la société s'intéresse de plus en plus à l'éthique.

La dernière décennie a vu émerger une énorme quantité de travaux de recherche sur l'éthique, des réflexions sur comment rendre les systèmes de vote plus démocratiques, l'encadrement des intelligences artificielles, le consentement éclairé des patients, le commerce équitable, etc.

À ceux qui sont pessimistes, je dirais qu'il y a peut-être plus d'espoir qu'ils pensent. Et si ce n'est pas le cas, il ne tient qu'à nous de changer les choses.

Quels domaines d'éthique vous intéressent le plus ?
{: .notice}

![P3R7]({{ site.baseurl }}/images/post-images/2020-12-04-ethique-normative/p3_r7.svg){: .medium height}

Les 4 domaines suscitant le plus d'intérêt sont l'éthique médicale, l'éthique de l'intelligence artificielle, la bioéthique et l'éthique sociale et politique.

J'aborderai des questions traitant de ces sujets et d'autres dans des articles futurs.

### Discussion

Les répondants au questionnaire sont tous issus de mon entourage et pourtant les réponses sont parfois très différentes. Ainsi même dans un groupe qui à priori partage un bon nombre de valeurs et de façons de voir le monde, on trouve des oppositions flagrantes sur certains points. Alors qu'en serait-il sur des questions bien plus complexes ?

S'il est déjà difficile de s'accorder sur le fait qu'il faille ou non dévier un train pour sauver des vies, qu'en sera-t-il lorsque les questions porteront sur des sujets plus complexes comme le tri des malades en pleine pandémie ou les règles d'engagement des forces armées ? Qu'en sera-t-il lorsque les questions porteront sur des sujets techniques sur lesquels la plupart des gens sont ignorants, comme les biais dans les algorithmes de big data ou les manipulations génétiques ?

Peut-être que la plupart des gens auront un sentiment de ce qui est juste ou non, mais s'il y a bien un point dont je suis conscient c'est qu'on peut croire quelque chose avec conviction, penser être dans le vrai et le juste, et se tromper complètement. D'où l'importance d'avoir une approche prudente, méthodique et minutieuse.

En regardant les réponses aux dilemmes éthiques de mon questionnaire, je me suis rendu compte de quelque chose d'important. Il est très certainement inutile de débattre de quelle réponse est la plus éthique, de quelle réponse doit être donnée à tel ou tel problème, tant qu'on n’a pas compris ce qui motive le choix de la position "adverse", tant qu'on n’a pas compris qu'il y a plusieurs manières d'aborder ces questions.

## Conclusion

Il est clair, en ce qui me concerne, que je privilégie une approche utilitariste dans la plupart des situations courantes, c'est-à-dire que je vais baser mes choix sur les conséquences prévisibles. Cependant je tiens quand même à un léger déontologisme.

Par exemple dans la variante 7 de l'exemple avec l'hôpital automatisé, le fait que le patient soit conscient d'être sauvé implique pour moi que tout prélèvement futur éventuel ne peut se faire sans son consentement. Je place donc ici le consentement d'un patient comme un principe déontologique.

Cependant j'estime qu'il ne s'agit pas d'un vrai déontologisme, mais d'une version "modérée", puisque je ne considère pas ce principe intransgressible absolument. Par exemple s'il se trouvait je ne sais comment que la mort de ce patient, sans son consentement, épargnerait aux trois quarts de la population mondiale une mort dans d'atroces souffrances, je crois que je transgresserai ce principe.

Il y a des cas qui sont pour moi particulièrement clairs (comme la variante 1), d'autres où j'hésite plus (comme la variante 6).

Ce que j'en retire, en plus d'une meilleure compréhension des approches déontologistes et utilitaristes, c'est que les questions éthiques peuvent en réalité être bien plus complexes qu'elles n'en ont l'air et qu'avec les mêmes faits on peut arriver à faire des choix très différents en fonction des théories d'éthique normative qu'on favorise et des critères qu'on utilise.

Je crois avoir toujours eu un avis sur un bon nombre de questions éthiques, mais j'ai décidé de le suspendre pour un temps car, pour être honnête, j'ai formé un jugement sur bon nombre de questions sans les étudier en profondeur, en étant partisan dès le départ et en cherchant des justifications aux conclusions auxquelles je voulais aboutir (X est bien, Y est mal...) plutôt que de chercher sans préjugés à suivre les faits et à aboutir à une conclusion, qu'elle me plaise ou non.

Je me suis rendu compte qu'il est facile d'avoir un avis très tranché sur des situations bien plus complexes. Ainsi je veux dorénavant, et pas uniquement dans le cadre de l'éthique, éviter de me dire "je ne comprends pas comment on peut croire/faire/soutenir telle ou telle chose". Si je ne comprends pas c'est ma faute : à moi de suspendre mon jugement, d'écouter les arguments, d'examiner les faits.

Je pense également devoir faire une distinction à l'avenir entre ce qui relève strictement de mes valeurs personnelles et ce que je souhaite appliquer à la société entière par des lois d'éthique.

Par exemple pour le premier dilemme de mon sondage, à la question de dévier ou non le train, je considère non éthique de ne pas le dévier. Cependant d'autres pensent l'inverse et je respecte leur point de vue, même si je ne suis pas d'accord. Je comprends leurs raisons même si elles ne me semblent, personnellement, pas bonnes. C'est pourquoi même si je pense que dévier le train est le choix moral, je ne veux pas de loi obligeant une personne dans cette situation à dévier le train. Je suis convaincu qu'il y a des sujets d'éthiques qui doivent être régulés et d'autres non.

Il a aussi un aspect de l'utilitarisme que j'aime bien et qui me fait réfléchir. Cette approche étant basée sur les conséquences globales, elle est plus tournée vers les autres que soi-même. Ainsi, pour schématiser grossièrement ce point, un utilitariste ne respectera pas tant la maxime "ne fais pas aux autres ce que tu ne veux pas qu'on te fasse", mais plutôt "ne fais pas aux autres ce qu'ils ne veulent pas qu'on leur fasse". En effet rien n'indique que l'autre a les mêmes préférences que moi. Quand je fais quelque chose à quelqu'un d'autre, ce qui importe n'est pas ce que j'aurais ressenti si on m'avait fait/dit ça, mais bien ce que l'autre a ressenti quand je lui ai fait/dit ça.

Un autre aspect de l'utilitarisme auquel je veux accorder de l'importance, c'est la règle de l'évaluation par les conséquences. Ainsi si je veux faire un don à une ONG pour lutter contre la faim dans le monde, j'aurais le devoir moral de le donner à l'ONG qui en fera l'usage le plus efficace. Ou si j'hésite entre les politiques économiques de deux candidats à la présidence, j'aurais le devoir moral de l'évaluer en fonction de ses impacts sur la population et non uniquement sur l'économie ou mes finances personnelles.

Après toutes ces réflexions, je veux avoir une approche éthique :

* **Principalement utilitariste**. Je veux en première approche et dans la plupart des cas, évaluer les conséquences des actions pour déterminer leur valeur éthique, et chercher à comprendre les problèmes dans leur ensemble, complexité et diversité.
* **Complétée par des notions d'équité**. Je veux favoriser des conséquences équitables pour le plus grand nombre.
* **Encadrée par un déontologisme modéré**. Je veux limiter certaines décisions par des principes déontologiques (comme le consentement d'un patient) sans que ces principes m'empêchent de prendre une décision purement utilitariste si j'estime la situation suffisamment grave.
* **Minimalement prescriptive**. Je ne veux pas que mes points de vue éthiques soient prescriptifs (sous forme de loi ou autre) sauf si j'estime qu'un mal substantiel peut être causé.
* **Évolutive**. Je veux pouvoir réviser mes jugements éthiques quand des éléments nouveaux font leur apparition ou que j'ai une compréhension meilleure de la situation.

## Quelques ressources

Avant de terminer, voici quelques ressources intéressantes sur l'éthique normative ou des sujets d'éthique appliquée. Cette liste est très loin d'être exhaustive : elle ne contient que les ressources que j'ai déjà consultées et trouvé intéressant. Peut-être serez-vous aussi intéressés.

### Vidéos

Sur l'éthique normative, par Thibaut Giraud (alias Monsieur Phi), docteur en philosophie et vulgarisateur :

* [CONSÉQUENTIALISME - Quel est le but de la morale ?](https://www.youtube.com/watch?v=3hCffvguLTQ)
* [TU DOIS ! — La loi morale selon Kant](https://www.youtube.com/watch?v=Hj7JDMlJjJE)
* [Faut-il trier les patients ? L'éthique médicale au temps du COVID-19](https://www.youtube.com/watch?v=CaaEGtFH4FE)
* [La philosophie de The Good Place](https://www.youtube.com/watch?v=9FFI9bwYuK0)

Sur l'éthique normative, par Michael Sandel, docteur en philosophie :

* [Justice : What's The Right Thing To Do?](https://www.youtube.com/watch?v=kBdfcR-8hEY)

Sur le concept d'équité, par Lê Nguyên Hoang, docteur en mathématiques et vulgarisateur :

* [Mais au juste, est-ce juste ?](https://www.youtube.com/watch?v=rgzWqAyZW7o)
* [Partager un gâteau, c'est pas du gâteau !](https://www.youtube.com/watch?v=kefptSDi0Es)
* [La méritocratie mathématique](https://www.youtube.com/watch?v=DHw-dvrAv58)

Sur l'éthique de l'intelligence artificielle, par Lê Nguyên Hoang, docteur en mathématiques et vulgarisateur :

* [Enjeux autour du développement de l'IA](https://www.youtube.com/watch?v=AnbVoZ9itvE)

Sur l'éthique de l'intelligence artificielle, par Hannah Fry, docteure en mathématiques :

* [Should Computers Run the World?](https://www.youtube.com/watch?v=Rzhpf1Ai7Z4)

### Livres

Sur l'éthique de l'intelligence artificielle, par Hannah Fry, docteure en mathématiques :

* [Hello World: How to be Human in the Age of the Machine](https://www.amazon.fr/Hello-World-How-Human-Machine/dp/1784163066)

Sur l'éthique de l'intelligence artificielle, par Jean-François Bonnefon, docteur en psychologie cognitive :

* [La voiture qui en savait trop : L'intelligence artificielle a-t-elle une morale ?](https://www.amazon.fr/voiture-savait-trop-Lintelligence-t-elle/dp/2379310394)

### Articles en ligne

Sur l'éthique normative, dans l'Encyclopédie de Philosophie de Stanford :

* [Deontological Ethics](https://plato.stanford.edu/entries/ethics-deontological/)
* [Consequentialism](https://plato.stanford.edu/entries/consequentialism/)

## À venir

Ce premier article sur l'éthique est maintenant terminé. Il n'y en aura pas d'autres pendant un petit moment, tant que je n'aurai pas maturé sur ces questions d'éthique normative.

Dans la suite je compte explorer de nouveau les questions d'éthique, sur des questions pratiques principalement.

Mais avant ça je vais me pencher sur d'autres sujets, comme des curiosités juridiques ou scientifiques, le fonctionnement des systèmes électoraux et comment on peut les améliorer, la création d'un jeu de société, l'apparition du genre grammatical et l'évolution de l'orthographe, le fonctionnement des algorithmes d'intelligence artificielle, etc.

À bientôt !

<!-- Références -->

[^thomson_76]: Thomson, J. J., & The Hegeler Institute. (1976). *Killing, Letting Die, and the Trolley Problem* Monist, 59(2), 204‑217. <https://doi.org/10.5840/monist197659224>

[^thomson_85]: Thomson, J. J. (1985). *The Trolley Problem*. The Yale Law Journal, 94(6), 1395. <https://doi.org/10.2307/796133>

[^britannica_morality_ethics]: Grannan, C. (s. d.). *What's the Difference Between Morality and Ethics?* Encyclopedia Britannica. Consulté le 20 novembre 2020, à l'adresse <https://www.britannica.com/story/whats-the-difference-between-morality-and-ethics>

[^axiome_2]: Axiome. (2017, 25 aout). *Optimisme probabiliste \| Axiome 2* [Vidéo]. YouTube. <https://www.youtube.com/watch?v=__11zlc8Fdo>

[^plato_deo]: Alexander, L., & Moore, M. (2020a). Deontological ethics. In E. N. Zalta (Éd.), *The Stanford Encyclopedia of Philosophy* (Winter 2020). Metaphysics Research Lab, Stanford University. <https://plato.stanford.edu/entries/ethics-deontological/>

[^brita_deo]: The Editors of Encyclopaedia Britannica. (2020). Deontological ethics. In *Encyclopædia Britannica*. <https://www.britannica.com/topic/deontological-ethics>

[^plato_con]: Sinnott-Armstrong, W. (2019). Consequentialism. In E. N. Zalta (Éd.), *The Stanford Encyclopedia of Philosophy* (Summer 2019). Metaphysics Research Lab, Stanford University. <https://plato.stanford.edu/entries/consequentialism/>

[^dual_processing]: Greene, J. D. (2014). *Beyond Point-and-Shoot Morality : Why Cognitive (Neuro)Science Matters for Ethics*. Ethics, 124(4), 695‑726. <https://doi.org/10.1086/675875>

[^deo_uti]: Conway, P., & Gawronski, B. (2013). *Deontological and utilitarian inclinations in moral decision making : A process dissociation approach*. Journal of Personality and Social Psychology, 104(2), 216‑235. <https://doi.org/10.1037/a0031021>
